{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializations\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading and preprocessing\n",
    "\n",
    "with open('breast-cancer-wisconsin.data', 'r') as f:\n",
    "    # removing the rows with missing values represented by '?'\n",
    "    data_raw = [l.strip('\\n').split(',') for l in f if '?' not in l]\n",
    "# storing the data values as integers\n",
    "data = np.array(data_raw).astype(int)\n",
    "\n",
    "# data contains 683 out of 699 instances of the original data and 11 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "target_depth = 6 # max depth allowed for the decision tree with pruning\n",
    "threshold_list = range(1, 11) # list of thresholds to be considered for splitting continuous features\n",
    "part_one_feature = [4] # list of features to be considered for part 1\n",
    "feature_list = [8, 10, 6, 9, 7, 2] # list of features to be considered for part 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions for part 1\n",
    "# Attribution: Hongtao Hao's sample solution for P2\n",
    "\n",
    "def entropy(data):\n",
    "    ''' Calculates the entropy of a given dataset'''\n",
    "    entropy = 0\n",
    "    count = len(data) # total number of instances\n",
    "    n2 = np.sum(data[:, -1] == 2) # number of k1\n",
    "    n4 = np.sum(data[:, -1] == 4) # number of k2\n",
    "    if n2 == 0 or n4 == 0: \n",
    "        return 0\n",
    "    else: \n",
    "        for n in [n2, n4]:\n",
    "            p = n/count\n",
    "            entropy += - (p * np.log2(p))\n",
    "        return entropy\n",
    "\n",
    "def infogain(data, feature, threshold):\n",
    "    ''' Calculates the information gain for a given feature and threshold'''\n",
    "    count = len(data)\n",
    "    d1 = data[data[:, feature - 1] <= threshold]\n",
    "    d2 = data[data[:, feature - 1] > threshold]\n",
    "    proportion_d1 = len(d1) / count\n",
    "    proportion_d2 = len(d2) / count\n",
    "    return entropy(data) - proportion_d1 * entropy(d1) - proportion_d2 * entropy(d2)\n",
    "\n",
    "\n",
    "def get_best_split(data, feature_list, threshold_list):\n",
    "    ''' Calculates Max Info Gain, computes the threshold and returns the feature, threshold, and predictions for left and right nodes'''\n",
    "    c = len(data)\n",
    "    \n",
    "    # c0 is the number of instances with class label 2\n",
    "    c0 = sum(b[-1] == 2 for b in data)\n",
    "\n",
    "    # if all instances have class label 2, return 2\n",
    "    # else if all instances have class label 4, return 4\n",
    "    if c0 == c: return 2, None, None, None\n",
    "    if c0 == 0: return 4, None, None, None\n",
    "\n",
    "    # compute possible information gain for all features and thresholds\n",
    "    # pairwise combinations\n",
    "    ig = [[infogain(\n",
    "        data, feature, threshold) for threshold in threshold_list] for feature in feature_list]\n",
    "    \n",
    "    # convert ig to numpy array\n",
    "    ig = np.array(ig)\n",
    "    \n",
    "    # find the maximum information gain\n",
    "    max_ig = max(max(i) for i in ig)\n",
    "\n",
    "    # if max_ig is 0, return 2 if c0 >= c - c0, else return 4\n",
    "    # remember c0 is the number of instances with class label 2\n",
    "    # and c - c0 is the number of instances with class label 4\n",
    "    if max_ig == 0:\n",
    "        if c0 >= c - c0:\n",
    "            return 2, None, None, None\n",
    "        else:\n",
    "            return 4, None, None, None\n",
    "\n",
    "    # can also return max_ig in case you need it for debugging\n",
    "    \n",
    "    # find the index of the maximum information gain\n",
    "    idx = np.unravel_index(np.argmax(ig, axis=None), ig.shape)\n",
    "\n",
    "    # return the feature, threshold, and predictions for left and right nodes\n",
    "    feature, threshold = feature_list[idx[0]], threshold_list[idx[1]]\n",
    "\n",
    "    # binary split: split the data into two parts based on the threshold\n",
    "    dl = data[data[:, feature - 1] <= threshold]\n",
    "    dr = data[data[:, feature - 1] > threshold]\n",
    "\n",
    "    # get the number of instances with class label 2 and 4 in the left node\n",
    "    dl_n2 = np.sum(dl[:,-1] == 2)\n",
    "    dl_n4 = np.sum(dl[:,-1] == 4)\n",
    "\n",
    "    # if the number of instances with class label 2 is greater than or equal to 4, predict 2\n",
    "    if dl_n2 >= dl_n4:\n",
    "        dl_prediction = 2\n",
    "    else:\n",
    "        # else predict 4\n",
    "        dl_prediction = 4\n",
    "    \n",
    "    # get the number of instances with class label 2 and 4 in the left node\n",
    "    dr_n2 = np.sum(dr[:,-1] == 2)\n",
    "    dr_n4 = np.sum(dr[:,-1] == 4)\n",
    "\n",
    "    # if the number of instances with class label 2 is greater than or equal to 4, predict 2\n",
    "    if dr_n2 >= dl_n4:\n",
    "        dr_prediction = 2\n",
    "    else:\n",
    "        # else predict 4\n",
    "        dr_prediction = 4\n",
    "    return feature, threshold, dl_prediction, dr_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances with class label 2:  444\n",
      "Number of instances with class label 4:  239\n"
     ]
    }
   ],
   "source": [
    "# question 1\n",
    "\n",
    "n2 = np.sum(data[:,-1] == 2)\n",
    "n4 = np.sum(data[:,-1] == 4)\n",
    "print('Number of instances with class label 2: ', n2)\n",
    "print('Number of instances with class label 4: ', n4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of the dataset:  0.9340026588217948\n"
     ]
    }
   ],
   "source": [
    "# question 2\n",
    "\n",
    "print('Entropy of the dataset: ', entropy(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 2s with feature value <= optimal threshold:  395\n",
      "Number of 2s with feature value > optimal threshold:  49\n",
      "Number of 4s with feature value <= optimal threshold:  9\n",
      "Number of 4s with feature value > optimal threshold:  230\n"
     ]
    }
   ],
   "source": [
    "feature = part_one_feature[0] - 1\n",
    "optimal_threshold = None\n",
    "max_ig = 0\n",
    "for threshold in threshold_list:\n",
    "    ig = infogain(data, feature, threshold)\n",
    "    if ig > max_ig:\n",
    "        max_ig = ig\n",
    "        optimal_threshold = threshold\n",
    "\n",
    "print('Number of 2s with feature value <= optimal threshold: ', np.sum(data[data[:, feature] <= optimal_threshold][:,-1] == 2))\n",
    "print('Number of 2s with feature value > optimal threshold: ', np.sum(data[data[:, feature] > optimal_threshold][:,-1] == 2))\n",
    "print('Number of 4s with feature value <= optimal threshold: ', np.sum(data[data[:, feature] <= optimal_threshold][:,-1] == 4))\n",
    "print('Number of 4s with feature value > optimal threshold: ', np.sum(data[data[:, feature] > optimal_threshold][:,-1] == 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information gain after split 0.5690253924531676\n"
     ]
    }
   ],
   "source": [
    "# question 4\n",
    "\n",
    "print('Information gain after split', infogain(data, 4, optimal_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions for part 2\n",
    "# Attribution: Hongtao Hao's sample solution for P2\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, feature = None, threshold = None, l_prediction = None, r_prediction = None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.l_prediction = l_prediction # prediction for left subtree\n",
    "        self.r_prediction = r_prediction # prediction for right subtree\n",
    "        self.l = None # left child or left subtree\n",
    "        self.r = None # right child or right subtree`\n",
    "        self.correct = 0 # number of correct predictions\n",
    "\n",
    "def split(data, node):\n",
    "    # split the data into two parts based on the threshold\n",
    "    feature, threshold = node.feature, node.threshold\n",
    "    d1 = data[data[:,feature-1] <= threshold]\n",
    "    d2 = data[data[:,feature-1] > threshold]\n",
    "    return (d1,d2)\n",
    "\n",
    "def create_tree(data, node, feature_list):\n",
    "    ''' Recursively creates the tree'''\n",
    "    d1,d2 = split(data, node)\n",
    "    f1, t1, l1_prediction, r1_prediction = get_best_split(d1, feature_list, threshold_list)\n",
    "    f2, t2, l2_prediction, r2_prediction = get_best_split(d2, feature_list, threshold_list)\n",
    "    if t1 == None: \n",
    "        node.l_pre = f1\n",
    "    else:\n",
    "        node.l = Node(f1, t1, l1_prediction, r1_prediction)\n",
    "        create_tree(d1, node.l, feature_list)\n",
    "    if t2 == None: \n",
    "        node.r_pre = f2\n",
    "    else:\n",
    "        node.r = Node(f2, t2, l2_prediction, r2_prediction)\n",
    "        create_tree(d2, node.r, feature_list)  \n",
    "    \n",
    "def maxDepth(node):\n",
    "    ''' Calculates the maximum depth of the tree'''\n",
    "    if node is None:\n",
    "        return 0 ;\n",
    "    else :\n",
    "        left_depth = maxDepth(node.l)\n",
    "        right_depth = maxDepth(node.r)\n",
    " \n",
    "        return max(left_depth, right_depth) + 1\n",
    "\n",
    "def expand_root(data, feature_list, threshold_list):\n",
    "    ''' Expands the root node'''\n",
    "    feature, threshold, dl, dr = get_best_split(\n",
    "        data, feature_list, threshold_list)\n",
    "    root = Node(feature, threshold)\n",
    "    # first split\n",
    "    data1, data2 = split(data, root)\n",
    "    create_tree(data, root, feature_list)\n",
    "    return root\n",
    "\n",
    "def tree_prediction(node, x):\n",
    "    ''' Predicts the class label for a single instance (test data)'''\n",
    "    feature = node.feature\n",
    "    threshold = node.threshold\n",
    "    l_prediction = node.l_prediction\n",
    "    r_prediction = node.r_prediction\n",
    "    l = node.l\n",
    "    r = node.r\n",
    "\n",
    "    if x[feature-1] <= threshold:\n",
    "        if l_prediction == x[-1]:\n",
    "            node.correct += 1\n",
    "        if l == None:\n",
    "            return l_prediction\n",
    "        else:\n",
    "            return tree_prediction(l, x)\n",
    "    else:\n",
    "        if r_prediction == x[-1]:\n",
    "            node.correct += 1\n",
    "        if r == None:\n",
    "            return r_prediction\n",
    "        else:\n",
    "            return tree_prediction(r, x)\n",
    "\n",
    "def print_tree(node, f, prefix=''):\n",
    "    fea = node.feature\n",
    "    t = node.threshold\n",
    "    l_pre = node.l_prediction\n",
    "    r_pre = node.r_prediction\n",
    "    l = node.l\n",
    "    r = node.r\n",
    "    if l == None:\n",
    "        f.write(prefix+'if (x'+str(fea)+') <= '+str(t)+') return '+str(l_pre)+'\\n')\n",
    "    else:\n",
    "        f.write(prefix+'if (x'+str(fea)+') <= '+str(t)+')\\n')\n",
    "        print_tree(l, f, prefix+' ')\n",
    "    if r == None:\n",
    "        f.write(prefix+'else return '+str(r_pre)+'\\n')\n",
    "    else:\n",
    "        f.write(prefix+'else\\n')\n",
    "        print_tree(r, f, prefix+' ')\n",
    "                \n",
    "\n",
    "def prune(node, depth):\n",
    "    ''' Prunes the tree to the specified depth'''\n",
    "    if depth == 1:\n",
    "        node.l = None\n",
    "        node.r = None\n",
    "    else:\n",
    "        if node.l != None:\n",
    "            prune(node.l, depth-1)\n",
    "        if node.r != None:\n",
    "            prune(node.r, depth-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 5\n",
    "\n",
    "# create a decision tree with the root node\n",
    "root = expand_root(data, feature_list, threshold_list)\n",
    "# print the tree to a file\n",
    "with open('tree.txt', 'w') as f:\n",
    "    print_tree(root, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum depth of the tree:  11\n"
     ]
    }
   ],
   "source": [
    "# question 6\n",
    "\n",
    "print('Maximum depth of the tree: ', maxDepth(root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 7\n",
    "\n",
    "# load the test data\n",
    "with open('test.txt', 'r') as f:\n",
    "    test_raw = [l.strip('\\n').split(',') for l in f if '?' not in l]\n",
    "test = np.array(test_raw).astype(int)\n",
    "\n",
    "# get all the predictions\n",
    "predictions = [tree_prediction(root, x) for x in test]\n",
    "\n",
    "# store the predictions in a file\n",
    "with open('predictions.txt', 'w') as f:\n",
    "    f.write(','.join([str(x) for x in predictions]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 8\n",
    "\n",
    "# prune the tree to the specified depth and store the pruned tree in a file\n",
    "prune(root, target_depth)\n",
    "with open('pruned_tree.txt', 'w') as f:\n",
    "    print_tree(root, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 9\n",
    "\n",
    "# get all the predictions on the pruned tree\n",
    "predictions = [tree_prediction(root, x) for x in test]\n",
    "\n",
    "# store the predictions in a file\n",
    "with open('pruned_predictions.txt', 'w') as f:\n",
    "    f.write(','.join([str(x) for x in predictions]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
