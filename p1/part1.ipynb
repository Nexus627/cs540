{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializations and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading and rescaling\n",
    "\n",
    "df = pd.read_csv('mnist_train.csv')\n",
    "x_train = (df.iloc[:, 1:].values) / 255\n",
    "y_train = df.iloc[:, 0].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "num_epochs = 200\n",
    "learning_rate = 0.01\n",
    "\n",
    "# number of features\n",
    "d = x_train.shape[1]\n",
    "\n",
    "# initialize weights and bias to uniform random values in the range [0, 1]\n",
    "w = np.random.rand(d)\n",
    "b = np.random.rand()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Training data of digits 2 and 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % Code attribution: Hongtao Hao's P1 example solution.\n",
    "\n",
    "test_labels = [2, 7]\n",
    "indices = np.where(np.isin(y_train, test_labels))[0]\n",
    "\n",
    "# get the indices of the training data that have labels 2 and 7\n",
    "x = x_train[indices]\n",
    "y = y_train[indices]\n",
    "\n",
    "# label 2 as 0 and label 7 as 1\n",
    "y[y == test_labels[0]] = 0\n",
    "y[y == test_labels[1]] = 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 41162.6741, Accuracy = 0.5126\n",
      "Epoch 2: Loss = 43283.0478, Accuracy = 0.4874\n",
      "Epoch 3: Loss = 3335.9799, Accuracy = 0.9603\n",
      "Epoch 4: Loss = 3013.5150, Accuracy = 0.9642\n",
      "Epoch 5: Loss = 2888.4124, Accuracy = 0.9651\n",
      "Epoch 6: Loss = 2769.2437, Accuracy = 0.9670\n",
      "Epoch 7: Loss = 2672.6696, Accuracy = 0.9679\n",
      "Epoch 8: Loss = 2585.9372, Accuracy = 0.9688\n",
      "Epoch 9: Loss = 2459.6528, Accuracy = 0.9703\n",
      "Epoch 10: Loss = 2364.9823, Accuracy = 0.9715\n",
      "Epoch 11: Loss = 2309.7920, Accuracy = 0.9723\n",
      "Epoch 12: Loss = 2233.8632, Accuracy = 0.9728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ddeka\\AppData\\Local\\Temp\\ipykernel_18968\\555718820.py:6: RuntimeWarning: overflow encountered in exp\n",
      "  a = 1 / (1 + np.exp(-a))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Loss = 2193.1333, Accuracy = 0.9738\n",
      "Epoch 14: Loss = 2158.3900, Accuracy = 0.9741\n",
      "Epoch 15: Loss = 2131.8123, Accuracy = 0.9745\n",
      "Epoch 16: Loss = 2073.9706, Accuracy = 0.9749\n",
      "Epoch 17: Loss = 2055.7008, Accuracy = 0.9753\n",
      "Epoch 18: Loss = 2040.5565, Accuracy = 0.9754\n",
      "Epoch 19: Loss = 1989.3853, Accuracy = 0.9759\n",
      "Epoch 20: Loss = 1961.1136, Accuracy = 0.9763\n",
      "Epoch 21: Loss = 1939.1888, Accuracy = 0.9767\n",
      "Epoch 22: Loss = 1911.1625, Accuracy = 0.9771\n",
      "Epoch 23: Loss = 1894.1781, Accuracy = 0.9773\n",
      "Epoch 24: Loss = 1861.5087, Accuracy = 0.9776\n",
      "Epoch 25: Loss = 1825.1608, Accuracy = 0.9780\n",
      "Epoch 26: Loss = 1817.9376, Accuracy = 0.9781\n",
      "Epoch 27: Loss = 1801.3908, Accuracy = 0.9785\n",
      "Epoch 28: Loss = 1769.1706, Accuracy = 0.9789\n",
      "Epoch 29: Loss = 1747.0769, Accuracy = 0.9791\n",
      "Epoch 30: Loss = 1729.6877, Accuracy = 0.9792\n",
      "Epoch 31: Loss = 1697.9886, Accuracy = 0.9795\n",
      "Epoch 32: Loss = 1669.4979, Accuracy = 0.9798\n",
      "Epoch 33: Loss = 1648.6192, Accuracy = 0.9800\n",
      "Epoch 34: Loss = 1631.2938, Accuracy = 0.9800\n",
      "Epoch 35: Loss = 1620.1169, Accuracy = 0.9804\n",
      "Epoch 36: Loss = 1604.9229, Accuracy = 0.9804\n",
      "Epoch 37: Loss = 1580.3282, Accuracy = 0.9810\n",
      "Epoch 38: Loss = 1561.9935, Accuracy = 0.9810\n",
      "Epoch 39: Loss = 1550.5577, Accuracy = 0.9813\n",
      "Epoch 40: Loss = 1523.8890, Accuracy = 0.9813\n",
      "Epoch 41: Loss = 1503.5927, Accuracy = 0.9819\n",
      "Epoch 42: Loss = 1488.4971, Accuracy = 0.9820\n",
      "Epoch 43: Loss = 1471.4314, Accuracy = 0.9820\n",
      "Epoch 44: Loss = 1457.8632, Accuracy = 0.9822\n",
      "Epoch 45: Loss = 1442.4919, Accuracy = 0.9823\n",
      "Epoch 46: Loss = 1429.1474, Accuracy = 0.9824\n",
      "Epoch 47: Loss = 1410.0131, Accuracy = 0.9824\n",
      "Epoch 48: Loss = 1394.7207, Accuracy = 0.9829\n",
      "Epoch 49: Loss = 1380.8161, Accuracy = 0.9831\n",
      "Epoch 50: Loss = 1368.0663, Accuracy = 0.9834\n",
      "Epoch 51: Loss = 1360.5081, Accuracy = 0.9835\n",
      "Epoch 52: Loss = 1353.1324, Accuracy = 0.9834\n",
      "Epoch 53: Loss = 1345.6893, Accuracy = 0.9834\n",
      "Epoch 54: Loss = 1338.7788, Accuracy = 0.9836\n",
      "Epoch 55: Loss = 1334.1454, Accuracy = 0.9837\n",
      "Epoch 56: Loss = 1331.1092, Accuracy = 0.9838\n",
      "Epoch 57: Loss = 1326.9094, Accuracy = 0.9837\n",
      "Epoch 58: Loss = 1320.3100, Accuracy = 0.9838\n",
      "Epoch 59: Loss = 1314.6909, Accuracy = 0.9838\n",
      "Epoch 60: Loss = 1310.7197, Accuracy = 0.9838\n",
      "Epoch 61: Loss = 1308.1695, Accuracy = 0.9840\n",
      "Epoch 62: Loss = 1306.6645, Accuracy = 0.9840\n",
      "Epoch 63: Loss = 1305.1109, Accuracy = 0.9841\n",
      "Epoch 64: Loss = 1302.2564, Accuracy = 0.9842\n",
      "Epoch 65: Loss = 1296.6442, Accuracy = 0.9841\n",
      "Epoch 66: Loss = 1291.0715, Accuracy = 0.9842\n",
      "Epoch 67: Loss = 1286.2962, Accuracy = 0.9842\n",
      "Epoch 68: Loss = 1281.7123, Accuracy = 0.9844\n",
      "Epoch 69: Loss = 1277.4205, Accuracy = 0.9845\n",
      "Epoch 70: Loss = 1272.1750, Accuracy = 0.9845\n",
      "Epoch 71: Loss = 1265.4484, Accuracy = 0.9845\n",
      "Epoch 72: Loss = 1258.6497, Accuracy = 0.9846\n",
      "Epoch 73: Loss = 1253.1087, Accuracy = 0.9845\n",
      "Epoch 74: Loss = 1247.6706, Accuracy = 0.9848\n",
      "Epoch 75: Loss = 1241.7314, Accuracy = 0.9848\n",
      "Epoch 76: Loss = 1232.5233, Accuracy = 0.9849\n",
      "Epoch 77: Loss = 1221.3086, Accuracy = 0.9849\n",
      "Epoch 78: Loss = 1209.6192, Accuracy = 0.9847\n",
      "Epoch 79: Loss = 1199.3062, Accuracy = 0.9849\n",
      "Epoch 80: Loss = 1191.6719, Accuracy = 0.9852\n",
      "Epoch 81: Loss = 1182.9113, Accuracy = 0.9853\n",
      "Epoch 82: Loss = 1173.4164, Accuracy = 0.9853\n",
      "Epoch 83: Loss = 1165.0885, Accuracy = 0.9855\n",
      "Epoch 84: Loss = 1158.4995, Accuracy = 0.9856\n",
      "Epoch 85: Loss = 1153.9189, Accuracy = 0.9856\n",
      "Epoch 86: Loss = 1149.7363, Accuracy = 0.9856\n",
      "Epoch 87: Loss = 1145.4518, Accuracy = 0.9858\n",
      "Epoch 88: Loss = 1140.0508, Accuracy = 0.9859\n",
      "Epoch 89: Loss = 1134.2923, Accuracy = 0.9859\n",
      "Epoch 90: Loss = 1127.1042, Accuracy = 0.9859\n",
      "Epoch 91: Loss = 1120.7343, Accuracy = 0.9860\n",
      "Epoch 92: Loss = 1114.0414, Accuracy = 0.9862\n",
      "Epoch 93: Loss = 1108.1011, Accuracy = 0.9863\n",
      "Epoch 94: Loss = 1102.1131, Accuracy = 0.9864\n",
      "Epoch 95: Loss = 1097.0118, Accuracy = 0.9865\n",
      "Epoch 96: Loss = 1092.7760, Accuracy = 0.9866\n",
      "Epoch 97: Loss = 1087.7353, Accuracy = 0.9865\n",
      "Epoch 98: Loss = 1082.9031, Accuracy = 0.9866\n",
      "Epoch 99: Loss = 1078.5644, Accuracy = 0.9867\n",
      "Epoch 100: Loss = 1075.2247, Accuracy = 0.9866\n",
      "Epoch 101: Loss = 1072.2607, Accuracy = 0.9866\n",
      "Epoch 102: Loss = 1069.7302, Accuracy = 0.9866\n",
      "Epoch 103: Loss = 1067.8137, Accuracy = 0.9868\n",
      "Epoch 104: Loss = 1065.2292, Accuracy = 0.9869\n",
      "Epoch 105: Loss = 1062.2130, Accuracy = 0.9869\n",
      "Epoch 106: Loss = 1057.9050, Accuracy = 0.9870\n",
      "Epoch 107: Loss = 1052.7926, Accuracy = 0.9868\n",
      "Epoch 108: Loss = 1048.2971, Accuracy = 0.9868\n",
      "Epoch 109: Loss = 1044.5076, Accuracy = 0.9870\n",
      "Epoch 110: Loss = 1040.8521, Accuracy = 0.9871\n",
      "Epoch 111: Loss = 1038.0591, Accuracy = 0.9872\n",
      "Epoch 112: Loss = 1034.9437, Accuracy = 0.9872\n",
      "Epoch 113: Loss = 1030.8931, Accuracy = 0.9871\n",
      "Epoch 114: Loss = 1026.8534, Accuracy = 0.9870\n",
      "Epoch 115: Loss = 1023.0440, Accuracy = 0.9871\n",
      "Epoch 116: Loss = 1019.4753, Accuracy = 0.9871\n",
      "Epoch 117: Loss = 1015.9523, Accuracy = 0.9871\n",
      "Epoch 118: Loss = 1013.1572, Accuracy = 0.9872\n",
      "Epoch 119: Loss = 1011.0261, Accuracy = 0.9874\n",
      "Epoch 120: Loss = 1009.3517, Accuracy = 0.9874\n",
      "Epoch 121: Loss = 1007.2426, Accuracy = 0.9874\n",
      "Epoch 122: Loss = 1005.5243, Accuracy = 0.9874\n",
      "Epoch 123: Loss = 1003.3677, Accuracy = 0.9873\n",
      "Epoch 124: Loss = 1001.3384, Accuracy = 0.9875\n",
      "Epoch 125: Loss = 999.5590, Accuracy = 0.9876\n",
      "Epoch 126: Loss = 997.7788, Accuracy = 0.9875\n",
      "Epoch 127: Loss = 996.0917, Accuracy = 0.9875\n",
      "Epoch 128: Loss = 994.4568, Accuracy = 0.9875\n",
      "Epoch 129: Loss = 993.0602, Accuracy = 0.9876\n",
      "Epoch 130: Loss = 991.9260, Accuracy = 0.9876\n",
      "Epoch 131: Loss = 991.0425, Accuracy = 0.9877\n",
      "Epoch 132: Loss = 989.2731, Accuracy = 0.9877\n",
      "Epoch 133: Loss = 987.4272, Accuracy = 0.9877\n",
      "Epoch 134: Loss = 985.6978, Accuracy = 0.9878\n",
      "Epoch 135: Loss = 984.0706, Accuracy = 0.9878\n",
      "Epoch 136: Loss = 982.1560, Accuracy = 0.9878\n",
      "Epoch 137: Loss = 979.9966, Accuracy = 0.9876\n",
      "Epoch 138: Loss = 977.8039, Accuracy = 0.9876\n",
      "Epoch 139: Loss = 974.4587, Accuracy = 0.9876\n",
      "Epoch 140: Loss = 970.8881, Accuracy = 0.9876\n",
      "Epoch 141: Loss = 967.0909, Accuracy = 0.9876\n",
      "Epoch 142: Loss = 961.7418, Accuracy = 0.9876\n",
      "Epoch 143: Loss = 955.7860, Accuracy = 0.9876\n",
      "Epoch 144: Loss = 949.7622, Accuracy = 0.9876\n",
      "Epoch 145: Loss = 944.2009, Accuracy = 0.9876\n",
      "Epoch 146: Loss = 937.8828, Accuracy = 0.9879\n",
      "Epoch 147: Loss = 931.1931, Accuracy = 0.9879\n",
      "Epoch 148: Loss = 925.1099, Accuracy = 0.9879\n",
      "Epoch 149: Loss = 919.4352, Accuracy = 0.9880\n",
      "Epoch 150: Loss = 914.0035, Accuracy = 0.9880\n",
      "Epoch 151: Loss = 909.4949, Accuracy = 0.9881\n",
      "Epoch 152: Loss = 905.8382, Accuracy = 0.9883\n",
      "Epoch 153: Loss = 902.5741, Accuracy = 0.9883\n",
      "Epoch 154: Loss = 899.8971, Accuracy = 0.9883\n",
      "Epoch 155: Loss = 897.7106, Accuracy = 0.9882\n",
      "Epoch 156: Loss = 896.0139, Accuracy = 0.9882\n",
      "Epoch 157: Loss = 894.7832, Accuracy = 0.9883\n",
      "Epoch 158: Loss = 894.0049, Accuracy = 0.9883\n",
      "Epoch 159: Loss = 893.5312, Accuracy = 0.9885\n",
      "Epoch 160: Loss = 893.2890, Accuracy = 0.9885\n",
      "Epoch 161: Loss = 892.6237, Accuracy = 0.9885\n",
      "Epoch 162: Loss = 891.0486, Accuracy = 0.9885\n",
      "Epoch 163: Loss = 889.0331, Accuracy = 0.9885\n",
      "Epoch 164: Loss = 887.1608, Accuracy = 0.9885\n",
      "Epoch 165: Loss = 885.1707, Accuracy = 0.9886\n",
      "Epoch 166: Loss = 883.2872, Accuracy = 0.9886\n",
      "Epoch 167: Loss = 881.7570, Accuracy = 0.9887\n",
      "Epoch 168: Loss = 880.4973, Accuracy = 0.9886\n",
      "Epoch 169: Loss = 879.2325, Accuracy = 0.9888\n",
      "Epoch 170: Loss = 878.2668, Accuracy = 0.9889\n",
      "Epoch 171: Loss = 877.4511, Accuracy = 0.9889\n",
      "Epoch 172: Loss = 876.7057, Accuracy = 0.9889\n",
      "Epoch 173: Loss = 875.8447, Accuracy = 0.9889\n",
      "Epoch 174: Loss = 874.5595, Accuracy = 0.9889\n",
      "Epoch 175: Loss = 873.2720, Accuracy = 0.9889\n",
      "Epoch 176: Loss = 872.1427, Accuracy = 0.9889\n",
      "Epoch 177: Loss = 870.2378, Accuracy = 0.9888\n",
      "Epoch 178: Loss = 868.0522, Accuracy = 0.9888\n",
      "Epoch 179: Loss = 866.0182, Accuracy = 0.9887\n",
      "Epoch 180: Loss = 863.7509, Accuracy = 0.9887\n",
      "Epoch 181: Loss = 861.6253, Accuracy = 0.9886\n",
      "Epoch 182: Loss = 859.6679, Accuracy = 0.9886\n",
      "Epoch 183: Loss = 857.7348, Accuracy = 0.9886\n",
      "Epoch 184: Loss = 855.8758, Accuracy = 0.9886\n",
      "Epoch 185: Loss = 853.9256, Accuracy = 0.9888\n",
      "Epoch 186: Loss = 851.3455, Accuracy = 0.9887\n",
      "Epoch 187: Loss = 848.3827, Accuracy = 0.9887\n",
      "Epoch 188: Loss = 845.3718, Accuracy = 0.9887\n",
      "Epoch 189: Loss = 842.0548, Accuracy = 0.9889\n",
      "Epoch 190: Loss = 838.8858, Accuracy = 0.9890\n",
      "Epoch 191: Loss = 835.8483, Accuracy = 0.9889\n",
      "Epoch 192: Loss = 832.7572, Accuracy = 0.9889\n",
      "Epoch 193: Loss = 829.5499, Accuracy = 0.9889\n",
      "Epoch 194: Loss = 826.5475, Accuracy = 0.9890\n",
      "Epoch 195: Loss = 823.1817, Accuracy = 0.9889\n",
      "Epoch 196: Loss = 819.6850, Accuracy = 0.9889\n",
      "Epoch 197: Loss = 816.4809, Accuracy = 0.9889\n",
      "Epoch 198: Loss = 813.6532, Accuracy = 0.9889\n",
      "Epoch 199: Loss = 811.1240, Accuracy = 0.9890\n",
      "Epoch 200: Loss = 809.0190, Accuracy = 0.9891\n"
     ]
    }
   ],
   "source": [
    "# Training for num_epochs\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # calculate activations of all datapoints\n",
    "    a = x @ w + b\n",
    "    a = 1 / (1 + np.exp(-a))\n",
    "\n",
    "    # bound items in a to avoid log(0):\n",
    "    a = np.clip(a, 0.001, 0.999)\n",
    "\n",
    "    # calculate gradients\n",
    "    grad_w = x.T @ (a - y)\n",
    "    grad_b = np.sum(a - y)\n",
    "\n",
    "    # update weights and biases\n",
    "    w -= learning_rate * grad_w\n",
    "    b -= learning_rate * grad_b\n",
    "\n",
    "    # calculate loss\n",
    "    loss = -np.sum(y * np.log(a) + (1 - y) * np.log(1 - a))\n",
    "\n",
    "    # calculate accuracy\n",
    "    accuracy = np.sum(y == (a > 0.5)) / len(y)\n",
    "\n",
    "    # log results\n",
    "    print(f'Epoch {epoch}: Loss = {loss:.4f}, Accuracy = {accuracy:.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1\n",
    "\n",
    "np.savetxt('q1.txt', x[0].reshape(1, -1), fmt='%.2f', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2\n",
    "\n",
    "params = np.concatenate((w, [b]))\n",
    "\n",
    "np.savetxt('q2.txt', params.reshape(1, -1), fmt='%.4f', delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ddeka\\AppData\\Local\\Temp\\ipykernel_18968\\72740118.py:9: RuntimeWarning: overflow encountered in exp\n",
      "  a = 1 / (1 + np.exp(-a))\n"
     ]
    }
   ],
   "source": [
    "# Question 3\n",
    "\n",
    "# load test data from assignment\n",
    "test = np.loadtxt(\"test.txt\", delimiter=\",\")\n",
    "test = test / 255.0\n",
    "\n",
    "# calculate activations\n",
    "a = test @ w + b\n",
    "a = 1 / (1 + np.exp(-a))\n",
    "\n",
    "np.savetxt('q3.txt', a.reshape(1, -1), fmt='%.4f', delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 4\n",
    "\n",
    "preds = np.array([1 if activation >= 0.5 else 0 for activation in a])\n",
    "\n",
    "np.savetxt('q4.txt', preds.reshape(1, -1), fmt='%d', delimiter=',')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
